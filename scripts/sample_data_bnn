#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu May 19 13:17:07 2022
random sampler
@author: cg588
"""

#%%

# Import modules

import sys
try: 
    sys.path.index('/home/cg588')
except ValueError:
    sys.path.append('/home/cg588')


from MIE_atlas.featurizers.molfeaturizer import MorganFPFeaturizer
from MIE_atlas.tests.uncertainty_metrics import *
from MIE_atlas.models.BayesianNN import BayesianNN
import numpy as np
import scipy.stats as st
import matplotlib.pyplot as plt
import tensorflow.compat.v2 as tf
import tensorflow_probability as tfp
import tqdm
import pandas as pd
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.decomposition import PCA


tf.enable_v2_behavior()
tfd = tfp.distributions





# HYPERPARAMETERS
rng_1 = 1989
rng_2 = 2020
validation_proportion = 0.25
neurons = 10
hidden_layers = 3
LR = 0.005
epochs = 200
batch_size= 100

# Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.
def posterior_mean_field(kernel_size, bias_size=0, dtype=None):
    n = kernel_size + bias_size
    c = np.log(np.expm1(1e-5))
    return tf.keras.Sequential([
        tfp.layers.VariableLayer(2 * n, dtype=dtype),
        tfp.layers.DistributionLambda(lambda t: tfd.Independent(
            tfd.Normal(loc=t[..., :n],
                       scale=1e-5 + tf.nn.softplus(c + t[..., n:])), 
            reinterpreted_batch_ndims=1)),
    ])

# Specify a non-trainable prior

def prior_not_trainable(kernel_size, bias_size=0, dtype=None):
    n = kernel_size + bias_size
    pi = .5
    return tf.keras.Sequential([
        tfp.layers.DistributionLambda(lambda t: tfd.Mixture(
            cat=tfd.Categorical(probs=[pi, 1. - pi]),
            components=[tfd.MultivariateNormalDiag(loc=tf.zeros(n), scale_diag=.001 * tf.ones(n)),
                        tfd.MultivariateNormalDiag(loc=tf.zeros(n), scale_diag=1.5 * tf.ones(n))
                        ])
                                      )
    ])

'''
def prior_not_trainable(kernel_size, bias_size=0, dtype=None):
    n = kernel_size + bias_size
    return tf.keras.Sequential([
            tfp.layers.DistributionLambda(lambda t: tfd.MultivariateNormalDiag(
                    loc=tf.zeros(n), scale_diag=tf.ones(n)
                )
            )
        ])
'''

def negloglik(y, rv_y):
    return -rv_y.log_prob(y)



# Specify model architecture
def build_model(hidden_layers): 
    if hidden_layers == 1:
        model_aleatoric_epistemic = BayesianNN([
            tfp.layers.DenseVariational(neurons, posterior_mean_field, prior_not_trainable, kl_weight=kl_loss_weight, activation='relu', name = 'dense1'),
            tfp.layers.DenseVariational(1 + 1, posterior_mean_field, prior_not_trainable, kl_weight=kl_loss_weight, name = 'mve'),
            tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[..., :1], scale=1e-5 + tf.math.softplus(1e-5 * t[..., 1:])), name = 'posterior') 
            ])
        return model_aleatoric_epistemic
    
    elif hidden_layers == 2:
        model_aleatoric_epistemic = BayesianNN([
            tfp.layers.DenseVariational(neurons, posterior_mean_field, prior_not_trainable, kl_weight=kl_loss_weight, activation='relu', name = 'dense1'),
            tfp.layers.DenseVariational(neurons, posterior_mean_field, prior_not_trainable, kl_weight=kl_loss_weight, activation='relu', name = 'dense2'),
            tfp.layers.DenseVariational(1 + 1, posterior_mean_field, prior_not_trainable, kl_weight=kl_loss_weight, name = 'mve'),
            tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[..., :1], scale=1e-5 + tf.math.softplus(1e-5 * t[..., 1:])), name = 'lambda') 
            ])
        return model_aleatoric_epistemic
        
    elif hidden_layers == 3:
        model_aleatoric_epistemic = BayesianNN([
            tfp.layers.DenseVariational(neurons, posterior_mean_field, prior_not_trainable, kl_weight=kl_loss_weight, activation='relu', name = 'dense1'),
            tfp.layers.DenseVariational(neurons, posterior_mean_field, prior_not_trainable, kl_weight=kl_loss_weight, activation='relu', name = 'dense2'),
            tfp.layers.DenseVariational(neurons, posterior_mean_field, prior_not_trainable, kl_weight=kl_loss_weight, activation='relu', name = 'dense3'),
            tfp.layers.DenseVariational(1 + 1, posterior_mean_field, prior_not_trainable, kl_weight=kl_loss_weight, name = 'mve'),
            tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[..., :1], scale=1e-5 + tf.math.softplus(1e-1 * t[..., 1:])), name = 'lambda') 
            ])
        return model_aleatoric_epistemic
        
    else:
        print("Number of hidden layers outside this model scope, please choose 1, 2, or 3")
        return None


# Define model scope

#target_list = ['AChE', 'ADORA2A', 'ADRB1', 'ADRB2', 'AR', 'CHRM1', 'CHRM2', 'CHRM3', 'DD1R', 'DD2R', 'EDNRA', 'HRH1', 'HTR2A', 'KCNH2', 'LCK', 'NR3C1', 'OPRD1', 'OPRM1', 'SLC6A2', 'SLC6A3', 'SLC6A4']
target_list = ['DD2R']

bnn_val_r2_list = []
bnn_val_mae_list = []

bnn_val_cali_r2_list = []
bnn_val_oracle_auc_list = []
bnn_val_eff_list = []
bnn_val_disp_list = []


bnn_val_alea_mean_list = []
bnn_val_alea_std_list = []
bnn_val_epis_mean_list = []
bnn_val_epis_std_list = []
bnn_val_spars_r2_list = []


print("Beginning model building...")
print("There are", len(target_list), "targets to build for.")


    
# Get data
    
for target in target_list: 
    
    print("Building uncertainty for target:", target)
    raw_data = pd.read_csv("/home/cg588/MIE_atlas/data/input_data/"+target+".csv")
    val_data = pd.read_csv("/home/cg588/MIE_atlas/data/ext_validation/"+target+".csv")
    model_path = "/home/cg588/MIE_atlas/models/saved_models/" + target + "_bnn"
    
    featurizer = MorganFPFeaturizer(radius = 2, nBits = 8000)
    X = featurizer.transform(raw_data['SMILES'])
    y = raw_data['P(Act)'].values
    X_val = featurizer.transform(val_data['SMILES'])
    y_val = val_data['P(Act)'].values
    
    assert y.shape[0] == X.shape[0]
    assert y_val.shape[0] == X_val.shape[0]
    
    X, y = shuffle(X, y, random_state=rng_1)
    '''
    sampled_indices = np.random.choice(X.shape[0], X.shape[0]//10, replace = False)
    X = X[sampled_indices]
    y = y[sampled_indices]
    '''
    '''
    # Optional PCA
    pca = PCA(n_components=10)
    pca.fit(X)
    X = pca.transform(X)
    X_val = pca.transform(X_val)
    '''
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = rng_2)
    
    
    
    # Note that the kl_loss_weight value here is 1 over the size of the entire dataset, not just the batch size.
    
    kl_loss_weight = 1 / X_train.shape[0]
    
    # Train model
    bnn = build_model(hidden_layers)
    bnn.compile(optimizer=tf.optimizers.Adam(learning_rate=LR), loss=negloglik, metrics=['mse'])
    lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', 
                                                       patience = 1, 
                                                       min_delta = 15, 
                                                       factor = tf.math.exp(-0.01))
    early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', 
                                                  patience = 30, 
                                                  min_delta = 1, restore_best_weights=True)
    history = bnn.fit(X_train, y_train, epochs=epochs, 
                      callbacks = [lr_schedule, early_stop], 
                      verbose=False, validation_data=(X_test, y_test))
    bnn.summary()
    
    # Save model to model_path ITS STILL BUGGED ZZZZZ
    #bnn.save(model_path, save_format = 'tf', save_traces = False)
    
    # Plot history of loss values
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
    
    
    # Plot history of MSE values
    plt.plot(history.history['mse'])
    plt.plot(history.history['val_mse'])
    plt.title('model MSE')
    plt.ylabel('MSE')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
    
    # Get val metrics
    
    y_mean_val, y_sigma_val = bnn.predict_normal_distr(X_val, n_samples=500)
    
    bnn_val_mae = mean_absolute_error(y_val, y_mean_val)
    bnn_val_r2 = r2_score(y_val, y_mean_val)
    bnn_val_cali_r2 = cali_r2_score(bnn, X_val, y_val)
    bnn_val_oracle_auc = oracle_error_auc(bnn, X_val, y_val, significance=0.32)
    bnn_val_eff = efficiency_score(bnn, X_val, y_val, significance=0.32)
    bnn_val_disp = dispersion_score(bnn, X_val, y_val, significance=0.32)
    bnn_val_alea_mean = np.mean(bnn.get_aleatoric(X_val))
    bnn_val_alea_std = np.std(bnn.get_aleatoric(X_val))
    bnn_val_epis_mean = np.mean(bnn.get_epistemic(X_val))
    bnn_val_epis_std = np.std(bnn.get_epistemic(X_val))
    bnn_val_spars_r2 = epis_data_sparsity_pearsonr(bnn, X_train, y_train, X_val, n_neighbors = 100)
    
    bnn_val_mae_list.append(bnn_val_mae)
    bnn_val_r2_list.append(bnn_val_r2)
    bnn_val_cali_r2_list.append(bnn_val_cali_r2)
    bnn_val_oracle_auc_list.append(bnn_val_oracle_auc)
    bnn_val_eff_list.append(bnn_val_eff)
    bnn_val_disp_list.append(bnn_val_disp)
    bnn_val_alea_mean_list.append(bnn_val_alea_mean)
    bnn_val_alea_std_list.append(bnn_val_alea_std)
    bnn_val_epis_mean_list.append(bnn_val_epis_mean)
    bnn_val_epis_std_list.append(bnn_val_epis_std)
    bnn_val_spars_r2_list.append(bnn_val_spars_r2)
    
    
    print("val cali curve")
    cali_curve(bnn, X_val, y_val)
    
    # Reset session
    tf.keras.backend.clear_session()


# Print results to sheet
metrics_list = ["BNN val MAE", "BNN val R2", "BNN val cali R2", "BNN val oracle error AUC", "BNN val efficiency", "BNN val dispersion", "BNN val aleatoric mean", "BNN val aleatoric std", "BNN val epistemic mean", "BNN val epistemic std", "BNN val data sparsity r2"]

print("END")

results_df = pd.DataFrame(np.column_stack([bnn_val_mae_list, bnn_val_r2_list, bnn_val_cali_r2_list, bnn_val_oracle_auc_list, bnn_val_eff_list, bnn_val_disp_list, bnn_val_alea_mean_list, bnn_val_alea_std_list, bnn_val_epis_mean_list, bnn_val_epis_std_list, bnn_val_spars_r2_list]).T, 
                          index = metrics_list, 
                          columns=target_list)

results_mean = results_df.mean(axis = 1)
results_std = results_df.std(axis=1)

results_df['MEAN'] = results_mean
results_df['STD'] = results_std
print(results_df)
'''
from datetime import date
today = date.today().strftime("%d_%m_%Y")
results_df.to_csv("/home/cg588/MIE_atlas/tests/bayesian_nn/"+today+"_a10th.csv")
'''

'''

# Inspect predictions
y_mean_val, y_sigma_val = bnn.predict_normal_distr(X_val, n_samples = 500)
y_mean_train, y_sigma_train = bnn.predict_normal_distr(X_train, n_samples = 500)
y_mean_test, y_sigma_test = bnn.predict_normal_distr(X_test, n_samples = 500)

# Plot experimental vs predicted values for validation data
plt.figure()
plt.scatter(y_val,y_mean_val, marker='o')
plt.plot([3, 10], [3, 10], c = 'k', lw = 2)
plt.show()


print("--------------------")
print("Training Set MAE:")
print(mean_absolute_error(y_train, y_mean_train))
print("Test Set MAE:")
print(mean_absolute_error(y_test, y_mean_test))
print("Validation Set MAE:")
print(mean_absolute_error(y_val, y_mean_val))


# End the cycle
tf.keras.backend.clear_session()

# Endgame
print("END")

'''
#%%


epis_data_sparsity_plot(bnn, X_train, y_train, X_val, n_neighbors = 50)
#%%

